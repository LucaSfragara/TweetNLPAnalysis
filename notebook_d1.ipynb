{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec1d0c1",
   "metadata": {},
   "source": [
    "# MIT 15.776: Hands-On Deep Learning\n",
    "## Final Project: Disaster Tweets Analysis \n",
    "\n",
    "**Giuseppe Iannone, Luca Sfragara, Trisha Sutivong, Hanna Zhang**\n",
    "\n",
    "This notebook contains our exploration of NLP models to classify tweets into real (1) and fake (0) announcements of disaster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fddbb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "keras.utils.set_random_seed(42) # setting seed \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e0f39e",
   "metadata": {},
   "source": [
    "### Step 0: Loading Data\n",
    "The data set comprises of 7,613 labeled tweets for training and 3,263 unlabeled tweets for evaluation. Each instance contains: \n",
    "- Text: The tweet content (max 280 characters)\n",
    "- Keyword: Optional disaster-related keyword (e.g., “wildfire”, “earthquake”)\n",
    "- Location: Optional user-provided location information\n",
    "- Target: Binary label (1 = real disaster, 0 = not a disaster)\n",
    "\n",
    "Because the test set given by competition is unlabeled, we will separate our training set into train, validation and test sets. We will use training set to train, validation to perform cross-validation and test set to evaluate our final performance in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd367a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:  5176\n",
      "validation set size:  1295\n",
      "test set size:  1142\n",
      "submission set size (not used in this assignment):  3263\n",
      "example data: \n",
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"nlp-getting-started/test.csv\")\n",
    "\n",
    "# separate out test set \n",
    "train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
    "    train_df['text'].values, \n",
    "    train_df['target'].values,\n",
    "    test_size=0.15,  # 15% for test set\n",
    "    random_state=42,\n",
    "    stratify=train_df['target'].values\n",
    ")\n",
    "\n",
    "# split remaining train into train (68%) and validation (17%)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_val_texts,\n",
    "    train_val_labels,\n",
    "    test_size=0.2,  # 20% of remaining\n",
    "    random_state=42,\n",
    "    stratify=train_val_labels\n",
    ")\n",
    "\n",
    "# print sizes of all sets\n",
    "print(\"train set size: \", len(train_texts))\n",
    "print(\"validation set size: \", len(val_texts))\n",
    "print(\"test set size: \", len(test_texts))\n",
    "print(\"submission set size (not used in this assignment): \", len(test_df))\n",
    "print(\"example data: \")\n",
    "print(train_df.head()) # looking at data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a066ac",
   "metadata": {},
   "source": [
    "### Step 1: Defining Evaluation Metric and Naive Baseline\n",
    "Following Kaggle's competition guidelines, we define four standard classifcation evaluation metrics we will use to assess our model performance: f1 score, accuracy, precision and recall. We define two functions for easy printing in the future and establish the baseline, i.e., model that predicts the most common class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def print_results(list_dicts):\n",
    "    models = [d['model'] for d in list_dicts]\n",
    "    print(f\"\\n{'Metric':<12}\", end=\"\")\n",
    "    for model in models:\n",
    "        print(f\"{model:<30}\", end=\"\")\n",
    "    print()\n",
    "    for metric in ['f1', 'accuracy', 'precision', 'recall']:\n",
    "        print(f\"{metric:<12}\", end=\"\")\n",
    "        for d in list_dicts:\n",
    "            print(f\"{d[metric]:<30.4f}\", end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b902e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution in data:\n",
      "fake: 4342 (57.03%)\n",
      "real: 3271 (42.97%)\n",
      "\n",
      "Metric      Most Common Class (Baseline)  Most Common Class (Baseline)  \n",
      "f1          0.0000                        0.0000                        \n",
      "accuracy    0.5701                        0.5701                        \n",
      "precision   0.0000                        0.0000                        \n",
      "recall      0.0000                        0.0000                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trishasutivong/.julia/conda/3/aarch64/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "master_list = [] # create list for future models\n",
    "# examine distribution of labels in data \n",
    "print(\"Target distribution in data:\")\n",
    "counts = train_df['target'].value_counts()\n",
    "percentages = train_df['target'].value_counts(normalize=True) * 100\n",
    "print(f\"fake: {counts[0]} ({percentages[0]:.2f}%)\")\n",
    "print(f\"real: {counts[1]} ({percentages[1]:.2f}%)\")\n",
    "\n",
    "# baseline - what is our accuracy score if we predict the most common class?\n",
    "most_common = train_df['target'].mode()[0]\n",
    "baseline_pred = np.full(len(test_labels), most_common)\n",
    "# evaluate baseline\n",
    "baseline_metrics = evaluate_model(test_labels, baseline_pred,\"Most Common Class (Baseline)\")\n",
    "master_list.append(baseline_metrics)\n",
    "print_results(master_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef77ef",
   "metadata": {},
   "source": [
    "### Step 2: NLP Models From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e51bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff84d259",
   "metadata": {},
   "source": [
    "### Step 3.1: Leveraging Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17388ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d281bff1",
   "metadata": {},
   "source": [
    "### Step 3.2: Leveraging Pre-Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391abe4c",
   "metadata": {},
   "source": [
    "### Step 4: Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b849d195",
   "metadata": {},
   "source": [
    "### Step 5: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f70d08",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
